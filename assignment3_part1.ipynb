{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCKQQGblOqt0",
        "outputId": "a5a2419b-df29-4d45-9852-8c5b4e16f89e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torchtext==0.15.2 in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.5.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (2025.7.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy torch==2.0.1 torchtext==0.15.2 nltk scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W1M_W8uQNGo"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slB_VjhWQe8z"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = tokenizer(text.lower())\n",
        "    return [t for t in tokens if t not in stop_words]\n",
        "\n",
        "df['tokens'] = df['review'].apply(tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCOSvMaCQe_7",
        "outputId": "b04ff406-4232-4234-db41-80d2e5f66790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "# Assuming df['tokens'] is a column of tokenized lists\n",
        "counter = Counter()\n",
        "for tokens in df['tokens']:\n",
        "    counter.update(tokens)\n",
        "\n",
        "# Use `vocab(...)` directly â€” no need for torchtext.vocab.vocab\n",
        "vocab_obj = vocab(counter, specials=[\"<unk>\", \"<pad>\"])\n",
        "vocab_obj.set_default_index(vocab_obj[\"<unk>\"])\n",
        "\n",
        "# Example usage\n",
        "print(vocab_obj[\"<pad>\"])\n",
        "print(vocab_obj[\"some_word\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YByHkOwMQfCz",
        "outputId": "95f3cb3a-c94e-4e4d-a2f3-f357435eb554"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-10-82445146.py\", line 14, in <cell line: 0>\n",
            "    df['padded'] = df['numerical'].apply(pad_input)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\", line 4924, in apply\n",
            "    ).apply()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\", line 1427, in apply\n",
            "    return self.apply_standard()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\", line 1507, in apply_standard\n",
            "    mapped = obj._map_values(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\", line 921, in _map_values\n",
            "    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\", line 1743, in map_array\n",
            "    return lib.map_infer(values, mapper, convert=convert)\n",
            "  File \"/tmp/ipython-input-10-82445146.py\", line 12, in pad_input\n",
            "    return torch.tensor(seq + [vocab_obj[\"<pad>\"]] * (MAX_LEN - len(seq)))\n",
            "/tmp/ipython-input-10-82445146.py:12: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  return torch.tensor(seq + [vocab_obj[\"<pad>\"]] * (MAX_LEN - len(seq)))\n"
          ]
        }
      ],
      "source": [
        "# Numericalize function\n",
        "def numericalize(tokens):\n",
        "    return [vocab_obj[token] for token in tokens]\n",
        "\n",
        "df['numerical'] = df['tokens'].apply(numericalize)\n",
        "\n",
        "# Pad sequences\n",
        "MAX_LEN = 300\n",
        "\n",
        "def pad_input(seq):\n",
        "    seq = seq[:MAX_LEN]\n",
        "    return torch.tensor(seq + [vocab_obj[\"<pad>\"]] * (MAX_LEN - len(seq)))\n",
        "\n",
        "df['padded'] = df['numerical'].apply(pad_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aIuwyQKQfFq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Inputs\n",
        "X = torch.stack(df['padded'].tolist())\n",
        "y = torch.tensor(df['sentiment'].tolist(), dtype=torch.long)\n",
        "\n",
        "# Convert to list for train_test_split\n",
        "X_list = X.tolist()\n",
        "y_list = y.tolist()\n",
        "\n",
        "# Train-test split\n",
        "X_train_list, X_test_list, y_train_list, y_test_list = train_test_split(X_list, y_list, test_size=0.2)\n",
        "\n",
        "# Convert back to tensors\n",
        "X_train = torch.tensor(X_train_list, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test_list, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train_list, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test_list, dtype=torch.long)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXRckY39RLMV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        embedding_dim = embedding_matrix.shape[1]\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=128, batch_first=True)\n",
        "        self.fc = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)                           # [batch_size, seq_len, emb_dim]\n",
        "        output, hidden = self.rnn(embedded)                    # hidden: [1, batch_size, hidden_dim]\n",
        "        out = self.fc(hidden.squeeze(0))                       # [batch_size, 1]\n",
        "        return torch.sigmoid(out).squeeze(1)                   # [batch_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd7m-_QxRLPb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        embedding_dim = embedding_matrix.shape[1]  # safer than hardcoding 100\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=128, batch_first=True)\n",
        "        self.fc = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)                      # [batch_size, seq_len, emb_dim]\n",
        "        _, (hidden, _) = self.lstm(embedded)              # hidden: [1, batch_size, 128]\n",
        "        out = self.fc(hidden.squeeze(0))                  # [batch_size, 1]\n",
        "        return torch.sigmoid(out).squeeze(1)              # [batch_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGH4jFoKRLSS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_model(model, loader, epochs=5):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(xb)  # shape: [batch_size]\n",
        "            loss = criterion(outputs, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy during training (optional)\n",
        "            preds = (outputs >= 0.5).float()\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        accuracy = correct / total * 100\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j62y5La4RLUs"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device).float()  # Ensure yb is float for comparison\n",
        "            outputs = model(xb)\n",
        "            preds = (outputs >= 0.5).float()\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tzi4_vsBRauV",
        "outputId": "2bf55bd3-1010-4d62-82bf-325a75f6b122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RNN...\n",
            "Epoch 1, Loss: 0.7032\n",
            "Epoch 2, Loss: 0.6154\n",
            "Epoch 3, Loss: 0.4966\n",
            "Epoch 4, Loss: 0.3175\n",
            "Epoch 5, Loss: 0.1441\n",
            "Accuracy: 49.00%\n",
            "\n",
            "Training LSTM...\n",
            "Epoch 1, Loss: 0.6913\n",
            "Epoch 2, Loss: 0.6488\n",
            "Epoch 3, Loss: 0.5712\n",
            "Epoch 4, Loss: 0.4147\n",
            "Epoch 5, Loss: 0.2134\n",
            "Accuracy: 53.00%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------\n",
        "# Device configuration\n",
        "# -------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# -------------------------\n",
        "# Dummy data (replace with real data)\n",
        "# -------------------------\n",
        "# Assume each input sequence is 100-dimensional and of length 50\n",
        "vocab_size = 5000\n",
        "embedding_dim = 100\n",
        "sequence_length = 50\n",
        "num_samples = 1000\n",
        "\n",
        "X = torch.randint(0, vocab_size, (num_samples, sequence_length))\n",
        "y = torch.randint(0, 2, (num_samples,)).float()\n",
        "\n",
        "# -------------------------\n",
        "# Embedding matrix (random for demo)\n",
        "# Replace with pretrained like GloVe if available\n",
        "# -------------------------\n",
        "embedding_matrix = torch.randn(vocab_size, embedding_dim)\n",
        "\n",
        "# -------------------------\n",
        "# Dataset and DataLoader\n",
        "# -------------------------\n",
        "dataset = TensorDataset(X, y)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# -------------------------\n",
        "# Model Definitions\n",
        "# -------------------------\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.rnn = nn.RNN(embedding_dim, 128, batch_first=True)\n",
        "        self.fc = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        return torch.sigmoid(self.fc(hidden.squeeze(0)))\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.lstm = nn.LSTM(embedding_dim, 128, batch_first=True)\n",
        "        self.fc = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, (hidden, _) = self.lstm(embedded)\n",
        "        return torch.sigmoid(self.fc(hidden.squeeze(0)))\n",
        "\n",
        "# -------------------------\n",
        "# Training function\n",
        "# -------------------------\n",
        "def train_model(model, loader, epochs=5):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(xb).squeeze()\n",
        "            loss = criterion(outputs, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Evaluation function\n",
        "# -------------------------\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device).float()\n",
        "            preds = (model(xb).squeeze() > 0.5).float()\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    accuracy = 100 * correct / total if total != 0 else 0\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# -------------------------\n",
        "# Run RNN\n",
        "# -------------------------\n",
        "print(\"Training RNN...\")\n",
        "rnn_model = RNNClassifier(embedding_matrix)\n",
        "train_model(rnn_model, train_loader)\n",
        "evaluate(rnn_model, test_loader)\n",
        "\n",
        "# -------------------------\n",
        "# Run LSTM\n",
        "# -------------------------\n",
        "print(\"\\nTraining LSTM...\")\n",
        "lstm_model = LSTMClassifier(embedding_matrix)\n",
        "train_model(lstm_model, train_loader)\n",
        "evaluate(lstm_model, test_loader)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
